Transformers - Auto-Regressive (Text Generation)

Ultra-lightweight DistilGPT2 Large Language Model (decoder-only) is from Hugging Face:
https://huggingface.co/distilbert/distilgpt2. So as the full GPT-2 model:
https://huggingface.co/openai-community/gpt2.

Goals:
- Explore Hugging Face's transformers package to implement language tasks such as text generation.
- Perform text preprocessing and tokenization prior to using transformers.
- Choose the appropriate tokenizer and model configurations.
- Test the model's text generation capabilities.
- Change model's temperature to get either more creative or more predictable outputs.
- Explore token selection strategies: Greedy Search, Beam Search, N-gram Penalty, Sampling.
